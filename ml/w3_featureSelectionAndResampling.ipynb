{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhiqQ+KB/xV/s71YM1xua6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smicho01/google-collab-notebooks/blob/main/ml/w3_featureSelectionAndResampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EHY-sPdD_dP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FEATURE SELECTION AND RESAMPLING (W3)\n",
        "\n"
      ],
      "metadata": {
        "id": "-BTxfQF4_ehd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Feature Selection"
      ],
      "metadata": {
        "id": "ijDeeWwm_gcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
        "\n",
        "# The code block below uses the chi-squared (ùëê‚Ñéùëñ2) statistical test for non-negative features to select\n",
        "# 4 of the best features from the Pima Indians onset of diabetes dataset.\n",
        "\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# load data\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# feature extraction\n",
        "test  = SelectKBest(score_func=chi2, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "\n",
        "# sumarize scores\n",
        "set_printoptions(precision=3)\n",
        "print(fit.scores_)\n",
        "features = fit.transform(X)\n",
        "\n",
        "# summarize selected features\n",
        "print(features[0:5,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKSXxBbz_nEO",
        "outputId": "4f65c817-bc9c-4901-94fb-9709895c2dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
            "[[148.    0.   33.6  50. ]\n",
            " [ 85.    0.   26.6  31. ]\n",
            " [183.    0.   23.3  32. ]\n",
            " [ 89.   94.   28.1  21. ]\n",
            " [137.  168.   43.1  33. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Recursive Feature Elimination"
      ],
      "metadata": {
        "id": "s0JPqsQEOfIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction with RFE\n",
        "\n",
        "# The code block below uses RFE with the logistic regression algorithm to select the top 3 features.\n",
        "# The choice of algorithm does not matter too much as long as it is skillful and consistent.\n",
        "\n",
        "from pandas import read_csv\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# load data\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "rfe = RFE(model, )\n",
        "fit = rfe.fit(X, Y)\n",
        "\n",
        "print (\"Num Features: %d\" % fit.n_features_)\n",
        "print (\"Selected Features: %s\" % fit.support_)\n",
        "print (\"Feature Ranking: %s\" % fit.ranking_ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a8cST6wL7aK",
        "outputId": "cfddc742-26c3-4e4b-a3c9-361ca4cc0087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Features: 4\n",
            "Selected Features: [ True  True False False False  True  True False]\n",
            "Feature Ranking: [1 1 2 4 5 1 1 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Reduction using Principal Component Analysis\n",
        "\n",
        "Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a\n",
        "compressed form. Generally this is called a data reduction technique. A property of PCA is that you\n",
        "can choose the number of dimensions or principal components in the transformed result. In the\n",
        "example below, we use PCA and select 3 principal components."
      ],
      "metadata": {
        "id": "tHMOMdoBOnwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction with PCA\n",
        "from pandas import read_csv\n",
        "from sklearn.decomposition import PCA\n",
        "# load data\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "pca = PCA(n_components=3)\n",
        "fit = pca.fit(X)\n",
        "# summarize components\n",
        "print (\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
        "print (fit.components_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUI59jKYOx_C",
        "outputId": "63ee856e-42ea-4617-cb8f-25b229c9671a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance: [0.88854663 0.06159078 0.02579012]\n",
            "[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02\n",
            "   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]\n",
            " [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02\n",
            "   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]\n",
            " [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01\n",
            "   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Importance\n",
        "\n",
        "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance\n",
        "of features. In the example below we construct a ùê∏ùë•ùë°ùëüùëéùëáùëüùëíùëíùë†ùê∂ùëôùëéùë†ùë†ùëñùëìùëñùëíùëü classifier for the Pima\n",
        "Indians onset of diabetes dataset."
      ],
      "metadata": {
        "id": "Y2l-wifqP77u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance with Extra Trees Classifier\n",
        "from pandas import read_csv\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# load data\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X, Y)\n",
        "print(model.feature_importances_) #  importance score for each attribute where the larger the score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgPLWOgwP9YD",
        "outputId": "1094a9a6-0a89-435d-821d-0873aca2e431"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10880883 0.23710398 0.09795178 0.08053766 0.07645399 0.13670099\n",
            " 0.118028   0.14441476]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.Resampling\n",
        "\n",
        "Resampling methods that allow you to make accurate estimates for how well your algorithm will perform on new data. I"
      ],
      "metadata": {
        "id": "u8eh_6lCTb7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split into Train and Test Sets\n",
        "\n",
        "Because of the speed, it is useful to use this approach when the algorithm you are investigating is slow to train. A downside of this technique is that it can have a high variance."
      ],
      "metadata": {
        "id": "fNz-WBbnTxam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "test_size_percentage = 0.33\n",
        "seed = 7\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size_percentage, random_state=seed)\n",
        "model = LogisticRegression(solver= 'liblinear')\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "result = model.score(X_test, Y_test)\n",
        "print(\"Accuracy: %.3f%%\" % (result * 100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTBfHAhiTyUm",
        "outputId": "09814ec4-4d78-4db9-fec7-4af0cba5135a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.591%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###K-fold Cross-Valudation\n",
        " - less variance as train-test split\n",
        " - After running cross-validation you end up with k dfferent performance scores that you can summarise using a mean and a standard deviation"
      ],
      "metadata": {
        "id": "xrcC-gKsxKRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
        "\n",
        "model = LogisticRegression(solver= 'liblinear')\n",
        "\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "\n",
        "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGukaXs1x96R",
        "outputId": "492a66ff-dfe1-42a5-c3a2-6636f8154f77"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 77.086% (5.091%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Leave One Out Cross-Validation\n",
        "\n",
        " - leave 1 sample out of the training set to test model agains it\n",
        " - more reasonable result than k-fold\n",
        " - comput. more expensive than k-fold\n",
        "\n",
        "Why does the standard deviation score have more variance than the k-fold cross-validation ? :\n",
        "\n",
        "LOOCV: If one data point has an outlier or is particularly difficult to predict, it can significantly impact the performance metric, resulting in high variability in the standard deviation score.\n",
        "\n",
        "CV: Since the test set in each fold contains more data points than in LOOCV, the estimate of the test error tends to be less variable.\n"
      ],
      "metadata": {
        "id": "BB9oJs-6zeXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "loocv = LeaveOneOut()\n",
        "model = LogisticRegression(solver = 'liblinear')\n",
        "results = cross_val_score(model, X, Y, cv=loocv)\n",
        "\n",
        "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean() * 100.0, results.std() * 100.0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USxcghA20DTi",
        "outputId": "b4cfdb6e-104f-4618-fa4a-905096e0f754"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 76.823% (42.196%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Repeated Random Test-Train Splits\n",
        "\n",
        "Another variation on k-fold cross-validation is to create a random split of the data like the train/test split described above but repeat the process of splitting and evaluation of the algorithm multiple times, like cross-validation.\n"
      ],
      "metadata": {
        "id": "VGzmQnW-2Mqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using Shuffle Split Cross Validation\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "filename = 'pima-indians-diabetes.data.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "n_splits = 10\n",
        "test_size = 0.33\n",
        "seed = 7\n",
        "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n",
        "model = LogisticRegression(solver = 'liblinear')\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
      ],
      "metadata": {
        "id": "4vPRIQTM2d_S",
        "outputId": "a225131a-2957-4063-ac65-9f2d6efd1610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 76.535% (1.691%)\n"
          ]
        }
      ]
    }
  ]
}